\section{Introduction}
The setting of online conversations has become ubiquitous and commonplace. 
Although this has provided new opportunities for civil discourse, online conversations have also become increasingly associated with negative conversational phenomenon such as toxicity, hate speech, and personal attacks~\citep{Cheng_Danescu-Niculescu-Mizil_Leskovec_2021}.
In particular, seemingly calm beginnings to online discussions can devolve into heated personal attacks. 
Prior work has termed this phenomenon as conversations gone awry (CGA)~\citep{zhang-etal-2018-conversations, chang-danescu-niculescu-mizil-2019-trouble}.

Predicting when conversations will go awry is a natural task to try and automate. 
Building a model that can effectively predict this phenomenon would be useful in online discussion forums such as Reddit or Wikipedia talk pages. 
If the model flags a conversation as potentially derailing, then moderators could step in and prevent users from further escalating the conversation. 
This would help make these online discussion forums a more friendly and welcoming space. 

Recent studies have focused on developing models for the task of forecasting when conversations will go awry~\citep{zhang-etal-2018-conversations, chang-danescu-niculescu-mizil-2019-trouble, Yuan_Singh_2023}. 
These models use a combination of linguistic markers (e.g., politeness) and unsupervised learning to make early predictions. 
The difficulty of this task lies in the early prediction aspect - the model is unable to see the actual personal attack before making its prediction. 
State-of-the-art (SOTA) models are only able to achieve an accuracy of slightly less than 70\% on CGA benchmarks~\citep{zhang-etal-2018-conversations, chang-danescu-niculescu-mizil-2019-trouble, Yuan_Singh_2023}. In this project, we analyze the behavior of 10 RoBERTa models (with random initialization), which represent the SOTA for CGA tasks.

This performance leaves much to be desired. 
The prior work that developed the models does not carefully examine how the model makes its predictions. 
Further understanding of the model and its shortcomings would pave the way for future improvements on the CGA task. 
In particular, this project investigates the models' use of the conversational context in making its predictions. 

We wanted to understand two main research questions.
\begin{enumerate}
    \item Is understanding the conversational context essential for SOTA model performance?
    \item Do SOTA models utilize the conversational context when making forecasts?
\end{enumerate}
This project answers these questions by modifying and creating new contexts for the models to make forecasts with. 
We could then see how the models' predictions changed as a result of the new context. 

We hypothesized that conversational context is not actually necessary for the models' performance. 
By testing this hypothesis in a no-context setting, we find that SOTA models do not need conversational context for their performance.
From this result, we further hypothesized that the models are not actually utilizing the context when making forecasts. 
To that end, we design the \textbf{Tra}jectory \textbf{Ma}tters (TraMa) behavioral test.
This test creates an artificial context that would lead to a calm conversation, even when the targeted utterance is highly escalatory.
We find that SOTA models do in fact utilize conversational contexts when making forecasts, but the impact of context is less pronounced when the utterance is highly escalatory.
These results suggest that the current CGA benchmark may not be difficult enough, and that a new benchmark is needed to help train and improve model performance. 
