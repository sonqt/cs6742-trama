\begin{abstract}
    Online discussions have become increasingly associated with negative conversational phenomena, including the escalation of seemingly calm conversations into heated personal attacks.
    Recent works have developed models to forecast when conversations will go awry.
    However, these models perform inadequately (the best models achieve less than 70\% accuracy on standard benchmarks) and have not been thoroughly analyzed to understand how they make their predictions.
    This project aims to better understand how these models use conversational context in making forecasts.
    In particular, we investigate whether the models \textit{need} to use the conversational context for the benchmarks, and whether they \textit{actually} utilize the context when making forecasts.
    We test this by modifying and creating new contexts for the models to make forecasts with.
    We find that the models do not need conversational context for their performance, but they do utilize the context when making forecasts.
    This suggests that the current benchmarks may not be difficult enough, and that a new benchmark is needed to help train and improve model performance.
\end{abstract}