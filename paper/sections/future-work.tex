\section{Future Work}

This project makes preliminary steps towards understanding the way in which SOTA models use conversational context for the CGA task.
However, there are several avenues for future work that ought to be explored to further understand the behavior of these models.

First, exploring different ChatGPT prompts for the TraMa test could provide more robust insight into the models' behavior.
In this project, we only used one prompt for generating the preceding utterance for the targeted statement.
Using different prompts could help to ensure the model is not overfitted for a single type of conversational context.
Similarly, expanding the TraMa test to the CGA-Wiki dataset would be a natural next step.
Second, developing the TraMa test to include not just the preceding utterance, but also a preceding conversation, could provide a more interesting context for the models to make forecasts with.
This would allow us to see how the models use the broader conversational context to make their predictions.

Finally, this project's results indicate that a new benchmark could be warranted for the CGA task.
Developing such a benchmark from either synthetic or real data would be valuable for improving the SOTA models.
Ideally, such a benchmark would include more challenging samples that require the models to use conversational context to make accurate forecasts.
Then training the models on this new benchmark could help to improve their performance on the CGA task in general.
