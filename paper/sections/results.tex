\section{TraMa Test}
In previous experiments, we observe that while the absence of conversational context significantly affects the utterance-level forecasts of the RoBERTa model, it has minimal impact on the conversation-level forecasts. We hypothesize that when the model encounters highly escalatory utterances—such as impolite confrontations or loaded language—it may largely disregard the broader conversational context.

To test our hypothesis, we design the \textbf{Tra}jectory \textbf{Ma}tters (TraMa) test, where we artificially manipulate the conversational context to alter the meaning and intent of highly escalatory utterances in the dataset, creating a more calm conversations.

First, we select highly escalatory utterances in the dataset based on the utterance-level forecasts of the models in the no-context setting. We select 2 different sets of utterances (calling strong and weak awry):
\begin{itemize}
    \item \textbf{Strong}: Utterances where all $10$ RoBERTa models in no-context settings forecast the utterance as going awry.

    \item \textbf{Weak}: Utterances where $6-7$ out of $10$ models forecast the utterance as awry.
\end{itemize}

Given an escalatory utterance, we use ChatGPT-turbo3.5 to construct a trajectory for a calm conversation that gradually leads to the targeted utterance. To begin, we prompt ChatGPT to summarize the topic of the ongoing conversation.

{\ttfamily
\begin{align*}
& \text{Write a ONE sentence summary of the}\\
& \text{topic of the conversation.}\\
& \{\text{User}_1\} : \{\text{Utterance}_1\}\\
& ...\\
& \{\text{User}_n\} : \{\text{Utterance}_n\}\\
\end{align*}
}

Next, using the summary and the escalatory utterance, we prompt ChatGPT to generate a preceding utterance for the targeted statement. This preceding utterance is designed to ensure that the meaning of the targeted utterance aligns with a calm and constructive conversation.

{\ttfamily
\begin{align*}
& \text{Suppose that I am telling you about a}\\
& \text{past conversation with another friend}\\
& \text{and you are a listener. In summary, that}\\ 
& \text{conversation is about \{summary\}.}\\
& \\
& \text{Which question/advice/opinion might you}\\
& \text{raise as a listener to the story I told}\\ 
& \text{so that I respond with the following}\\ 
& \text{utterance.}\\ 
\end{align*}
}

Table~\ref{tab:trama-example} presents a pair of samples from our TraMa test. In the real conversation from the dataset, the discussion is likely to derail due to User3's dismissive and mocking response ("No you don't lol"), which invalidates User2's personal experience and comes across as disrespectful. This response could trigger defensiveness or offense from User2, increasing the likelihood of a personal attack as emotions escalate. However, when we replace User2's response with a calm, opinion-seeking utterance generated by ChatGPT, RoBERTa models still predict the conversation as likely to derail.

\input{tables/trama_examples}

Using the TraMa test, we can validate our hypothesis that the model, when encountering highly escalatory utterances, may largely overlook the broader conversational context. This can be assessed by measuring the difference in the probability of derailment between the real sample and the ChatGPT-generated TraMa sample, as demonstrated by the two samples in Table~\ref{tab:trama-example}. 

Figure~\ref{fig:trama-results} presents the results of our analysis using the TraMa behavioral test. The findings suggest that while context influences CGA model predictions on CMV, its impact is minimal when the utterance belongs to the Strong awry set. Notably, all 10 RoBERTa models consistently predict such samples as going awry. This is evident in Figure~\ref{fig:trama-results}, where the samples from the Strong set are densely clustered around the $(0, 0)$ point. These observations strongly support our initial hypothesis.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/trama_results.png}
    \caption{
        Changes in RoBERTa model forecast probabilities due to the absence of conversational context (Normal --- No-Context) and the TraMa change of conversational context (Normal --- TraMa).}
    \label{fig:trama-results}
\end{figure}