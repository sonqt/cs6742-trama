\section{Related Work}
Our work relates to prior research on anti-social behaviors in online 
communities, conversation forecasting, and constructing adversarial examples
to evaluate language systems.

\xhdr{Online anti-social behaviors}
Prior work extensively characterized anti-social behaviors in online communities.
%
These examples of anti-social behaviors include hate speech ~\citep{rottger-etal-2021-hatecheck, elsherief-etal-2021-latent}, 
toxicity~\citep{pavlopoulos-etal-2020-toxicity, bespalov-etal-2023-towards}, trolling~\citep{mojica-de-la-vega-ng-2018-modeling, lee-etal-2022-elf22}, etc. 
%
\citet{zhang-etal-2018-conversations} introduced the Conversations Gone Awry (CGA)
task that characterizes an awry conversation as one that derails into rude or 
hostile behaviors such as personal attacks. 
%
Our work focuses on this dimension of anti-social interactions.

\xhdr{Conversation forecasting}
Unlike conversation classification tasks, which focus on categorizing the end 
state of a conversation, forecasting entails predicting a future outcome in 
the conversation. 
%
Therefore, conversational forecasting has the potential to anticipate behaviors 
and inform decision-making in real-time.
%
Previous work also focused on conversational forecasting for toxic behaviors. 
%
In particular, \citet{chang-danescu-niculescu-mizil-2019-trouble} formalizes this on the CGA 
task. 
%
This is done in an online prediction manner, where forecasts are made at every 
utterance timestep.
%
Conversation-level forecasts are then determined using these utterance-level forecasts.
%
In this work, we follow the task setup introduced in 
\citet{chang-danescu-niculescu-mizil-2019-trouble}.
%
However, we further investigate the CGA conversational forecasting task 
by examining whether or not SOTA models use the context of 
the conversation in order to make its forecasts.


\xhdr{Adversarial Examples}
Adversarial attacks include crafted changes to the input to create more challenging
examples for the model to handle.
%
These examples could potentially mislead the models in making incorrect predictions.
%
Previous work used these attacks to understand model weaknesses and evaluate its 
robustness.
%
These attacks, applied in many other NLP tasks,  can be created by heuristically editing \cite{wang-bansal-2018-robust} or through neural-based generation \cite{iyyer-etal-2018-adversarial, khashabi-etal-2020-bang, bartolo-etal-2021-improving, fu-etal-2023-scene}.
%
However, our work uses adversaries to evaluate language understanding in the context of 
the CGA task. 
%
In particular, we use these adversarial examples to understand whether or not the 
SOTA models in the CGA task use conversational dynamics to make their predictions.
