\section{Related Work}
Our work relates to prior research on anti-social behaviors in online 
communities, converational forecasting, and constructing adversarial examples
to evaluate language systems.

\xhdr{Online anti-social behaviors}
Prior work extensively characterized anti-social behaviors in online communities.
%
These examples of anti-social behaviors include hate speech ~\citep{}, 
toxicity~\citep{}, trolling~\citep{}, etc. 
%
\citep{zhang-etal-2018-conversations} introduced the Conversations Gone Awry (CGA)
task that characterizes an awry conversation as one that derails into rude or 
hostile behaviors such as personal attacks. 
%
Our work focuses on this dimension of anti-social interactions.

\xhdr{Conversation forecasting}
Unlike conversation classification tasks, which focus on categorizing the state of 
a conversation, forecasting entails predicting a future outcome in the conversation. 
%
Therefore, conversational forecasting has the potential to anticipate behaviors 
and inform decision-making in real-time.
%
Previous work has focused on conversational forecasting for toxic behaviors\cite{}. 
%
\cite{chang-danescu-niculescu-mizil-2019-trouble} formalizes this task on the CGA 
task. 
%
This is done in an online prediction manner, where forecasts are made at every 
utterance timestep.
%
Conversation-level forecasts are then determined using these uterance-level forecasts.
%
In this work, we follow the task setup introduced in 
\cite{chang-danescu-niculescu-mizil-2019-trouble}.
%
However, we further investigate the CGA conversational forecasting task 
by examining whether or not SOTA models for this task use the context of 
the conversation in order to make its forecasts.


\xhdr{Adversarial Examples}
Adversarial attacks include crafted changes to the input to create more challenging
examples for the model to handle.
%
These examples could potentially mislead the models in making incorrect predictions.
%
Previous work used these attacks to understand model weaknesses and evaluate its 
robustness.
%
These attacks were applied in many other NLP tasks including question-answering\citep{}, 
text classification\cite{}, and machine translation\cite{}, etc.
%
However, our work uses adversaries to evaluate language understanding in the context of 
the CGA task. 
%
In particular, we use these adversarial examples to understand whether or not the 
SOTA models in the CGA task use conversational dynamics to make their predictions.
